<section class="bg-white small-padding" id="Projects">
    <div class="container no-gutter">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Projects</h2>
                <hr class="primary">
            </div>
        </div>
        <div class="row ">
            <blockquote class="h4 text-left">
                <p style="color:steelblue; font-size:14pt">Accompaniment Arrangement via Phrase Selection and Style Transfer</p>
            </blockquote>
            <div class="col-sm-4">
                <a href="accomontage_demo" class="portfolio-box">
                    <img src="archive/projects/project_accomontage.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline" >
                    <ul class='text-justify'>
                        <li><p> Accompaniment generation for lead melodies is an interesting topic in computer music. While conventional methods rely on seq2seq models, we propose a novel framework based on rule-based optimization and neural style transfer for high-quality generation.  </p></li>

                        <li><p>Our proposed <b><em>AccoMontage</em></b> system generates long-term and structured accompaniments for full-length songs, with state-of-the-art quality outperforming the baselines.</p></li>

                        <li><p> <a href="https://arxiv.org/abs/2108.11213" target="_blank">Paper</a> | 
                                <a href="accomontage_demo" target="_blank">Demo</a> | 
                                <a href="presentation_page#accomontage_presentation" target="_blank">Presentation</a> |
                                <a href="https://github.com/zhaojw1998/AccoMontage" target="_blank">Github Page</a>
                            </p>
                        </li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>

        <div class="row ">
            <blockquote class="h4 text-left">
                <p style="color:steelblue; font-size:14pt">Adversarial Explicitly-Constrained Conditional VAE</p>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-adversarial_ec2vae" class="portfolio-box">
                    <img src="archive/projects/project_ec2vae.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline" >
                    <ul class='text-justify'>
                        <li><p> One of the key issues of music generation is to control the generation process. EC2-VAE is a chord controlled melody generation model which disentangles the melody into pitch and rhythm. However, the modelâ€™s latent pitch encoding is not a chord-invariant representation, and thus cannot induce the network to learn a chord-melody dependency. </p></li>

                        <li><p>This project introduces adversarial training to detach chord cues from pitch, and contributes a novel disentanglement methodology for controllable generation.</p></li>

                        <li><p> <a href="project-adversarial_ec2vae" target="_blank">Demo Page</a> |
                                <a href="https://github.com/zhaojw1998/Adversarial-EC2-VAE--Melody-Generation-towards-Better-Chord-Control" target="_blank">Github Page</a>
                            </p>
                        </li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>
        
        <div class="row">
            <blockquote class="h4 text-left">
                <p style="color:steelblue; font-size:14pt">Real-Time Music-Driven Dance Generation for Humanoid Robot</p>
            </blockquote>
            <div class="col-sm-4">
                <a href="project-dancing_robot" class="portfolio-box">
                    <img src="archive/projects/project_robot.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Demo Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-sm-8">                
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>This project aims to implement a real-time music-driven robotic dancer, and we propose to generate improvisational dance sequence with Unit Selection Methodology based on two criteria: semantic relevance of dance units, and unit transition stability of the robot.</p></li>

                        <li><p>To render real-time performance, we apply Madmom for beat tracking and synchronize our dance to the music through a feedback loop with PID control.</p></li>

                        <li><p><a href="project-dancing_robot" target="_blank">Demo Page</a> |
                               <a href="presentation_page#dancing_robot_presentation" target="_blank">Presentation</a> |
                               <a href="https://github.com/zhaojw1998/Real-Time-Music-Driven-Dancing-Robot" target="_blank">Github Page</a>
                            </p>
                        </li>
                    </ul>
                </ul>
            </div>
        </div>

        <p></p>

        <div class="row">
            <blockquote class="h4 text-left">
                <p style="color:steelblue; font-size:14pt">Video Action Recognition via Cross-Modality Learning</p>
            </blockquote>
            <div class="col-sm-4">
                <a href="https://github.com/zhaojw1998/SoundtrackClassification" class="portfolio-box">
                    <img src="archive/projects/project_video.jpg" class="img-responsive" alt="">
                    <div class="portfolio-box-caption">
                        <div class="portfolio-box-caption-content">
                            <div class="project-category text-faded">
                                Project
                            </div>
                            <div class="project-name">
                                Github Page
                            </div>
                        </div>
                    </div>
                </a>
            </div>

            <div class="col-sm-8">
                <ul class="list-inline">
                    <ul class='text-justify'>
                        <li><p>Video Action Recognition is a popular computer vision task usually conducted exclusively on the image stream of the video. This project aims to explore the soundtrack and fuse the visual and auditory information for better recognition accuracy.</p></li>
                        <li><p>By training classification model with mel-spectrogram inputs, and through late fusion with the vision-based model prediction, we acquire a gain of 1.5% in recognition accuracy.
                        <li><p><a href="https://github.com/zhaojw1998/SoundtrackClassification" target="_blank">Github Page</a></p></li>
                    </ul>
                </ul>     
            </div>
        </div>

        <p></p>

    </div>
    
</section>
